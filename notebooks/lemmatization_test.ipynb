{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "\n",
    "from auto_diagnostic.lemmatization import Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['As frases de tópico são semelhantes às declarações de mini tese.\\\n",
    "        Como uma declaração de tese, um tópico frasal tem um \\\n",
    "        ponto principal. Considerando que a tese é o ponto principal do ensaio',\\\n",
    "        'o tópico frasal é o ponto principal do parágrafo.\\\n",
    "        Como a declaração de tese, um tópico frasal tem uma função unificadora. \\\n",
    "        Mas uma declaração de tese ou frase de tópico por si só map(lambda x: tokenizer.tokenize(x), text)não garante unidade.', \\\n",
    "        'Um ensaio é unificado se todos os parágrafos se relacionam com a tese,\\\n",
    "        um parágrafo é unificado se todas as sentenças se relacionam com o tópico frasal while.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_text = list(map(lambda x: tokenizer.tokenize(x), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['As',\n",
       "  'frases',\n",
       "  'de',\n",
       "  'tópico',\n",
       "  'são',\n",
       "  'semelhantes',\n",
       "  'às',\n",
       "  'declarações',\n",
       "  'de',\n",
       "  'mini',\n",
       "  'tese',\n",
       "  'Como',\n",
       "  'uma',\n",
       "  'declaração',\n",
       "  'de',\n",
       "  'tese',\n",
       "  'um',\n",
       "  'tópico',\n",
       "  'frasal',\n",
       "  'tem',\n",
       "  'um',\n",
       "  'ponto',\n",
       "  'principal',\n",
       "  'Considerando',\n",
       "  'que',\n",
       "  'a',\n",
       "  'tese',\n",
       "  'é',\n",
       "  'o',\n",
       "  'ponto',\n",
       "  'principal',\n",
       "  'do',\n",
       "  'ensaio'],\n",
       " ['o',\n",
       "  'tópico',\n",
       "  'frasal',\n",
       "  'é',\n",
       "  'o',\n",
       "  'ponto',\n",
       "  'principal',\n",
       "  'do',\n",
       "  'parágrafo',\n",
       "  'Como',\n",
       "  'a',\n",
       "  'declaração',\n",
       "  'de',\n",
       "  'tese',\n",
       "  'um',\n",
       "  'tópico',\n",
       "  'frasal',\n",
       "  'tem',\n",
       "  'uma',\n",
       "  'função',\n",
       "  'unificadora',\n",
       "  'Mas',\n",
       "  'uma',\n",
       "  'declaração',\n",
       "  'de',\n",
       "  'tese',\n",
       "  'ou',\n",
       "  'frase',\n",
       "  'de',\n",
       "  'tópico',\n",
       "  'por',\n",
       "  'si',\n",
       "  'só',\n",
       "  'map',\n",
       "  'lambda',\n",
       "  'x',\n",
       "  'tokenizer',\n",
       "  'tokenize',\n",
       "  'x',\n",
       "  'text',\n",
       "  'não',\n",
       "  'garante',\n",
       "  'unidade'],\n",
       " ['Um',\n",
       "  'ensaio',\n",
       "  'é',\n",
       "  'unificado',\n",
       "  'se',\n",
       "  'todos',\n",
       "  'os',\n",
       "  'parágrafos',\n",
       "  'se',\n",
       "  'relacionam',\n",
       "  'com',\n",
       "  'a',\n",
       "  'tese',\n",
       "  'um',\n",
       "  'parágrafo',\n",
       "  'é',\n",
       "  'unificado',\n",
       "  'se',\n",
       "  'todas',\n",
       "  'as',\n",
       "  'sentenças',\n",
       "  'se',\n",
       "  'relacionam',\n",
       "  'com',\n",
       "  'o',\n",
       "  'tópico',\n",
       "  'frasal',\n",
       "  'while']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8417f8e044be4c65a41c65e8c0ae3152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-14 04:40:02 INFO: Loading these models for language: pt (Portuguese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | bosque  |\n",
      "| mwt       | bosque  |\n",
      "| lemma     | bosque  |\n",
      "=======================\n",
      "\n",
      "2022-05-14 04:40:02 INFO: Use device: gpu\n",
      "2022-05-14 04:40:02 INFO: Loading: tokenize\n",
      "2022-05-14 04:40:02 INFO: Loading: mwt\n",
      "2022-05-14 04:40:02 INFO: Loading: lemma\n",
      "2022-05-14 04:40:02 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'frase', 'de', 'tópico', 'ser', 'semelhante', 'às', 'declaração', 'de', 'mini', 'tese', 'como', 'um', 'declaração', 'de', 'tese', 'um', 'tópico', 'frasal', 'ter', 'um', 'ponto', 'principal', 'considerar', 'que', 'o', 'tese', 'ser', 'o', 'ponto', 'principal', 'do', 'ensaio', 'o', 'tópico', 'frasal', 'ser', 'o', 'ponto', 'principal', 'do', 'parágrafo', 'como', 'o', 'declaração', 'de', 'tese', 'um', 'tópico', 'frasal', 'ter', 'um', 'função', 'unificadora', 'mas', 'um', 'declaração', 'de', 'tese', 'ou', 'frase', 'de', 'tópico', 'por', 'si', 'só', 'map', 'lambda', 'x', 'tokenizer', 'tokenize', 'x', 'text', 'não', 'garantir', 'unidade', 'um', 'ensaio', 'ser', 'unificado', 'se', 'todo', 'o', 'parágrafo', 'se', 'relacionam', 'com', 'o', 'tese', 'um', 'parágrafo', 'ser', 'unificado', 'se', 'todo', 'o', 'sentença', 'se', 'relacionam', 'com', 'o', 'tópico', 'frasal', 'while']\n"
     ]
    }
   ],
   "source": [
    "model = Lemmatization()\n",
    "lemmatized_list = model.lemmatize(tokenized_text)\n",
    "\n",
    "print(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7598cb7ea9dde01de882b849b5b3fab1d8679c9ba86ddaed5ab6c7288771da54"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
