{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from nltk import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['As frases de tópico são semelhantes às declarações de mini tese.\\\n",
    "        Como uma declaração de tese, um tópico frasal tem um \\\n",
    "        ponto principal. Considerando que a tese é o ponto principal do ensaio',\\\n",
    "        'o tópico frasal é o ponto principal do parágrafo.\\\n",
    "        Como a declaração de tese, um tópico frasal tem uma função unificadora. \\\n",
    "        Mas uma declaração de tese ou frase de tópico por si só map(lambda x: tokenizer.tokenize(x), text)não garante unidade.', \\\n",
    "        'Um ensaio é unificado se todos os parágrafos se relacionam com a tese,\\\n",
    "        um parágrafo é unificado se todas as sentenças se relacionam com o tópico frasal while.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_text = list(map(lambda x: tokenizer.tokenize(x), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['As',\n",
       "  'frases',\n",
       "  'de',\n",
       "  'tópico',\n",
       "  'são',\n",
       "  'semelhantes',\n",
       "  'às',\n",
       "  'declarações',\n",
       "  'de',\n",
       "  'mini',\n",
       "  'tese',\n",
       "  'Como',\n",
       "  'uma',\n",
       "  'declaração',\n",
       "  'de',\n",
       "  'tese',\n",
       "  'um',\n",
       "  'tópico',\n",
       "  'frasal',\n",
       "  'tem',\n",
       "  'um',\n",
       "  'ponto',\n",
       "  'principal',\n",
       "  'Considerando',\n",
       "  'que',\n",
       "  'a',\n",
       "  'tese',\n",
       "  'é',\n",
       "  'o',\n",
       "  'ponto',\n",
       "  'principal',\n",
       "  'do',\n",
       "  'ensaio'],\n",
       " ['o',\n",
       "  'tópico',\n",
       "  'frasal',\n",
       "  'é',\n",
       "  'o',\n",
       "  'ponto',\n",
       "  'principal',\n",
       "  'do',\n",
       "  'parágrafo',\n",
       "  'Como',\n",
       "  'a',\n",
       "  'declaração',\n",
       "  'de',\n",
       "  'tese',\n",
       "  'um',\n",
       "  'tópico',\n",
       "  'frasal',\n",
       "  'tem',\n",
       "  'uma',\n",
       "  'função',\n",
       "  'unificadora',\n",
       "  'Mas',\n",
       "  'uma',\n",
       "  'declaração',\n",
       "  'de',\n",
       "  'tese',\n",
       "  'ou',\n",
       "  'frase',\n",
       "  'de',\n",
       "  'tópico',\n",
       "  'por',\n",
       "  'si',\n",
       "  'só',\n",
       "  'não',\n",
       "  'garante',\n",
       "  'unidade'],\n",
       " ['Um',\n",
       "  'ensaio',\n",
       "  'é',\n",
       "  'unificado',\n",
       "  'se',\n",
       "  'todos',\n",
       "  'os',\n",
       "  'parágrafos',\n",
       "  'se',\n",
       "  'relacionam',\n",
       "  'com',\n",
       "  'a',\n",
       "  'tese',\n",
       "  'um',\n",
       "  'parágrafo',\n",
       "  'é',\n",
       "  'unificado',\n",
       "  'se',\n",
       "  'todas',\n",
       "  'as',\n",
       "  'sentenças',\n",
       "  'se',\n",
       "  'relacionam',\n",
       "  'com',\n",
       "  'o',\n",
       "  'tópico',\n",
       "  'frasal',\n",
       "  'while']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a837209477f485abf26eeafea0dce04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 01:40:46 INFO: Loading these models for language: pt (Portuguese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | bosque  |\n",
      "| mwt       | bosque  |\n",
      "| lemma     | bosque  |\n",
      "=======================\n",
      "\n",
      "2022-05-12 01:40:46 INFO: Use device: gpu\n",
      "2022-05-12 01:40:46 INFO: Loading: tokenize\n",
      "2022-05-12 01:40:46 INFO: Loading: mwt\n",
      "2022-05-12 01:40:46 INFO: Loading: lemma\n",
      "2022-05-12 01:40:46 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: As,\t\tLemma: o\n",
      "Text: frases,\t\tLemma: frase\n",
      "Text: de,\t\tLemma: de\n",
      "Text: tópico,\t\tLemma: tópico\n",
      "Text: são,\t\tLemma: ser\n",
      "Text: semelhantes,\t\tLemma: semelhante\n",
      "Text: às,\t\tLemma: às\n",
      "Text: declarações,\t\tLemma: declaração\n",
      "Text: de,\t\tLemma: de\n",
      "Text: mini,\t\tLemma: mini\n",
      "Text: tese,\t\tLemma: tese\n",
      "Text: Como,\t\tLemma: como\n",
      "Text: uma,\t\tLemma: um\n",
      "Text: declaração,\t\tLemma: declaração\n",
      "Text: de,\t\tLemma: de\n",
      "Text: tese,\t\tLemma: tese\n",
      "Text: um,\t\tLemma: um\n",
      "Text: tópico,\t\tLemma: tópico\n",
      "Text: frasal,\t\tLemma: frasal\n",
      "Text: tem,\t\tLemma: ter\n",
      "Text: um,\t\tLemma: um\n",
      "Text: ponto,\t\tLemma: ponto\n",
      "Text: principal,\t\tLemma: principal\n",
      "Text: Considerando,\t\tLemma: considerar\n",
      "Text: que,\t\tLemma: que\n",
      "Text: a,\t\tLemma: o\n",
      "Text: tese,\t\tLemma: tese\n",
      "Text: é,\t\tLemma: ser\n",
      "Text: o,\t\tLemma: o\n",
      "Text: ponto,\t\tLemma: ponto\n",
      "Text: principal,\t\tLemma: principal\n",
      "Text: do,\t\tLemma: do\n",
      "Text: ensaio,\t\tLemma: ensaio\n",
      "Text: o,\t\tLemma: o\n",
      "Text: tópico,\t\tLemma: tópico\n",
      "Text: frasal,\t\tLemma: frasal\n",
      "Text: é,\t\tLemma: ser\n",
      "Text: o,\t\tLemma: o\n",
      "Text: ponto,\t\tLemma: ponto\n",
      "Text: principal,\t\tLemma: principal\n",
      "Text: do,\t\tLemma: do\n",
      "Text: parágrafo,\t\tLemma: parágrafo\n",
      "Text: Como,\t\tLemma: como\n",
      "Text: a,\t\tLemma: o\n",
      "Text: declaração,\t\tLemma: declaração\n",
      "Text: de,\t\tLemma: de\n",
      "Text: tese,\t\tLemma: tese\n",
      "Text: um,\t\tLemma: um\n",
      "Text: tópico,\t\tLemma: tópico\n",
      "Text: frasal,\t\tLemma: frasal\n",
      "Text: tem,\t\tLemma: ter\n",
      "Text: uma,\t\tLemma: um\n",
      "Text: função,\t\tLemma: função\n",
      "Text: unificadora,\t\tLemma: unificadora\n",
      "Text: Mas,\t\tLemma: mas\n",
      "Text: uma,\t\tLemma: um\n",
      "Text: declaração,\t\tLemma: declaração\n",
      "Text: de,\t\tLemma: de\n",
      "Text: tese,\t\tLemma: tese\n",
      "Text: ou,\t\tLemma: ou\n",
      "Text: frase,\t\tLemma: frase\n",
      "Text: de,\t\tLemma: de\n",
      "Text: tópico,\t\tLemma: tópico\n",
      "Text: por,\t\tLemma: por\n",
      "Text: si,\t\tLemma: si\n",
      "Text: só,\t\tLemma: só\n",
      "Text: não,\t\tLemma: não\n",
      "Text: garante,\t\tLemma: garantir\n",
      "Text: unidade,\t\tLemma: unidade\n",
      "Text: Um,\t\tLemma: um\n",
      "Text: ensaio,\t\tLemma: ensaio\n",
      "Text: é,\t\tLemma: ser\n",
      "Text: unificado,\t\tLemma: unificado\n",
      "Text: se,\t\tLemma: se\n",
      "Text: todos,\t\tLemma: todo\n",
      "Text: os,\t\tLemma: o\n",
      "Text: parágrafos,\t\tLemma: parágrafo\n",
      "Text: se,\t\tLemma: se\n",
      "Text: relacionam,\t\tLemma: relacionam\n",
      "Text: com,\t\tLemma: com\n",
      "Text: a,\t\tLemma: o\n",
      "Text: tese,\t\tLemma: tese\n",
      "Text: um,\t\tLemma: um\n",
      "Text: parágrafo,\t\tLemma: parágrafo\n",
      "Text: é,\t\tLemma: ser\n",
      "Text: unificado,\t\tLemma: unificado\n",
      "Text: se,\t\tLemma: se\n",
      "Text: todas,\t\tLemma: todo\n",
      "Text: as,\t\tLemma: o\n",
      "Text: sentenças,\t\tLemma: sentença\n",
      "Text: se,\t\tLemma: se\n",
      "Text: relacionam,\t\tLemma: relacionam\n",
      "Text: com,\t\tLemma: com\n",
      "Text: o,\t\tLemma: o\n",
      "Text: tópico,\t\tLemma: tópico\n",
      "Text: frasal,\t\tLemma: frasal\n",
      "Text: while,\t\tLemma: while\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(\n",
    "    lang='pt',\n",
    "    processors='tokenize,mwt,lemma',\n",
    "    tokenize_pretokenized=True\n",
    ")\n",
    "doc = nlp(tokenized_text)\n",
    "\n",
    "print(*[f'Text: {word.text},\\t\\tLemma: {word.lemma}' for sentence in doc.sentences for word in sentence.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7598cb7ea9dde01de882b849b5b3fab1d8679c9ba86ddaed5ab6c7288771da54"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
