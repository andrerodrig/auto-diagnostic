{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "\n",
    "from auto_diagnostic.lemmatization import Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['As frases de tópico são semelhantes às declarações de mini tese.\\\n",
    "        Como uma declaração de tese, um tópico frasal tem um \\\n",
    "        ponto principal. Considerando que a tese é o ponto principal do ensaio',\\\n",
    "        'o tópico frasal é o ponto principal do parágrafo.\\\n",
    "        Como a declaração de tese, um tópico frasal tem uma função unificadora. \\\n",
    "        Mas uma declaração de tese ou frase de tópico por si só map(lambda x: tokenizer.tokenize(x), text)não garante unidade.', \\\n",
    "        'Um ensaio é unificado se todos os parágrafos se relacionam com a tese,\\\n",
    "        um parágrafo é unificado se todas as sentenças se relacionam com o tópico frasal while.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_text = list(map(lambda x: tokenizer.tokenize(x), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['As',\n",
       "  'frases',\n",
       "  'de',\n",
       "  'tópico',\n",
       "  'são',\n",
       "  'semelhantes',\n",
       "  'às',\n",
       "  'declarações',\n",
       "  'de',\n",
       "  'mini',\n",
       "  'tese',\n",
       "  'Como',\n",
       "  'uma',\n",
       "  'declaração',\n",
       "  'de',\n",
       "  'tese',\n",
       "  'um',\n",
       "  'tópico',\n",
       "  'frasal',\n",
       "  'tem',\n",
       "  'um',\n",
       "  'ponto',\n",
       "  'principal',\n",
       "  'Considerando',\n",
       "  'que',\n",
       "  'a',\n",
       "  'tese',\n",
       "  'é',\n",
       "  'o',\n",
       "  'ponto',\n",
       "  'principal',\n",
       "  'do',\n",
       "  'ensaio'],\n",
       " ['o',\n",
       "  'tópico',\n",
       "  'frasal',\n",
       "  'é',\n",
       "  'o',\n",
       "  'ponto',\n",
       "  'principal',\n",
       "  'do',\n",
       "  'parágrafo',\n",
       "  'Como',\n",
       "  'a',\n",
       "  'declaração',\n",
       "  'de',\n",
       "  'tese',\n",
       "  'um',\n",
       "  'tópico',\n",
       "  'frasal',\n",
       "  'tem',\n",
       "  'uma',\n",
       "  'função',\n",
       "  'unificadora',\n",
       "  'Mas',\n",
       "  'uma',\n",
       "  'declaração',\n",
       "  'de',\n",
       "  'tese',\n",
       "  'ou',\n",
       "  'frase',\n",
       "  'de',\n",
       "  'tópico',\n",
       "  'por',\n",
       "  'si',\n",
       "  'só',\n",
       "  'map',\n",
       "  'lambda',\n",
       "  'x',\n",
       "  'tokenizer',\n",
       "  'tokenize',\n",
       "  'x',\n",
       "  'text',\n",
       "  'não',\n",
       "  'garante',\n",
       "  'unidade'],\n",
       " ['Um',\n",
       "  'ensaio',\n",
       "  'é',\n",
       "  'unificado',\n",
       "  'se',\n",
       "  'todos',\n",
       "  'os',\n",
       "  'parágrafos',\n",
       "  'se',\n",
       "  'relacionam',\n",
       "  'com',\n",
       "  'a',\n",
       "  'tese',\n",
       "  'um',\n",
       "  'parágrafo',\n",
       "  'é',\n",
       "  'unificado',\n",
       "  'se',\n",
       "  'todas',\n",
       "  'as',\n",
       "  'sentenças',\n",
       "  'se',\n",
       "  'relacionam',\n",
       "  'com',\n",
       "  'o',\n",
       "  'tópico',\n",
       "  'frasal',\n",
       "  'while']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f89cbfeb8242eca282726409801a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-14 11:15:17 INFO: Loading these models for language: pt (Portuguese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | bosque  |\n",
      "| mwt       | bosque  |\n",
      "| lemma     | bosque  |\n",
      "=======================\n",
      "\n",
      "2022-05-14 11:15:17 INFO: Use device: gpu\n",
      "2022-05-14 11:15:17 INFO: Loading: tokenize\n",
      "2022-05-14 11:15:17 INFO: Loading: mwt\n",
      "2022-05-14 11:15:17 INFO: Loading: lemma\n",
      "2022-05-14 11:15:17 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: As \tlemma: o\n",
      "word: frases \tlemma: frase\n",
      "word: de \tlemma: de\n",
      "word: tópico \tlemma: tópico\n",
      "word: são \tlemma: ser\n",
      "word: semelhantes \tlemma: semelhante\n",
      "word: às \tlemma: às\n",
      "word: declarações \tlemma: declaração\n",
      "word: de \tlemma: de\n",
      "word: mini \tlemma: mini\n",
      "word: tese \tlemma: tese\n",
      "word: Como \tlemma: como\n",
      "word: uma \tlemma: um\n",
      "word: declaração \tlemma: declaração\n",
      "word: de \tlemma: de\n",
      "word: tese \tlemma: tese\n",
      "word: um \tlemma: um\n",
      "word: tópico \tlemma: tópico\n",
      "word: frasal \tlemma: frasal\n",
      "word: tem \tlemma: ter\n",
      "word: um \tlemma: um\n",
      "word: ponto \tlemma: ponto\n",
      "word: principal \tlemma: principal\n",
      "word: Considerando \tlemma: considerar\n",
      "word: que \tlemma: que\n",
      "word: a \tlemma: o\n",
      "word: tese \tlemma: tese\n",
      "word: é \tlemma: ser\n",
      "word: o \tlemma: o\n",
      "word: ponto \tlemma: ponto\n",
      "word: principal \tlemma: principal\n",
      "word: do \tlemma: do\n",
      "word: ensaio \tlemma: ensaio\n",
      "word: o \tlemma: o\n",
      "word: tópico \tlemma: tópico\n",
      "word: frasal \tlemma: frasal\n",
      "word: é \tlemma: ser\n",
      "word: o \tlemma: o\n",
      "word: ponto \tlemma: ponto\n",
      "word: principal \tlemma: principal\n",
      "word: do \tlemma: do\n",
      "word: parágrafo \tlemma: parágrafo\n",
      "word: Como \tlemma: como\n",
      "word: a \tlemma: o\n",
      "word: declaração \tlemma: declaração\n",
      "word: de \tlemma: de\n",
      "word: tese \tlemma: tese\n",
      "word: um \tlemma: um\n",
      "word: tópico \tlemma: tópico\n",
      "word: frasal \tlemma: frasal\n",
      "word: tem \tlemma: ter\n",
      "word: uma \tlemma: um\n",
      "word: função \tlemma: função\n",
      "word: unificadora \tlemma: unificadora\n",
      "word: Mas \tlemma: mas\n",
      "word: uma \tlemma: um\n",
      "word: declaração \tlemma: declaração\n",
      "word: de \tlemma: de\n",
      "word: tese \tlemma: tese\n",
      "word: ou \tlemma: ou\n",
      "word: frase \tlemma: frase\n",
      "word: de \tlemma: de\n",
      "word: tópico \tlemma: tópico\n",
      "word: por \tlemma: por\n",
      "word: si \tlemma: si\n",
      "word: só \tlemma: só\n",
      "word: map \tlemma: map\n",
      "word: lambda \tlemma: lambda\n",
      "word: x \tlemma: x\n",
      "word: tokenizer \tlemma: tokenizer\n",
      "word: tokenize \tlemma: tokenize\n",
      "word: x \tlemma: x\n",
      "word: text \tlemma: text\n",
      "word: não \tlemma: não\n",
      "word: garante \tlemma: garantir\n",
      "word: unidade \tlemma: unidade\n",
      "word: Um \tlemma: um\n",
      "word: ensaio \tlemma: ensaio\n",
      "word: é \tlemma: ser\n",
      "word: unificado \tlemma: unificado\n",
      "word: se \tlemma: se\n",
      "word: todos \tlemma: todo\n",
      "word: os \tlemma: o\n",
      "word: parágrafos \tlemma: parágrafo\n",
      "word: se \tlemma: se\n",
      "word: relacionam \tlemma: relacionam\n",
      "word: com \tlemma: com\n",
      "word: a \tlemma: o\n",
      "word: tese \tlemma: tese\n",
      "word: um \tlemma: um\n",
      "word: parágrafo \tlemma: parágrafo\n",
      "word: é \tlemma: ser\n",
      "word: unificado \tlemma: unificado\n",
      "word: se \tlemma: se\n",
      "word: todas \tlemma: todo\n",
      "word: as \tlemma: o\n",
      "word: sentenças \tlemma: sentença\n",
      "word: se \tlemma: se\n",
      "word: relacionam \tlemma: relacionam\n",
      "word: com \tlemma: com\n",
      "word: o \tlemma: o\n",
      "word: tópico \tlemma: tópico\n",
      "word: frasal \tlemma: frasal\n",
      "word: while \tlemma: while\n"
     ]
    }
   ],
   "source": [
    "model = Lemmatization()\n",
    "lemmatized_list = model.lemmatize(tokenized_text)\n",
    "\n",
    "# print(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7598cb7ea9dde01de882b849b5b3fab1d8679c9ba86ddaed5ab6c7288771da54"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
